{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Intelligence COM3240\n",
    "\n",
    "## Lab 8: Reinforcement Learning\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The main characteristic of reinforcement learning is that learning comes from interaction. The learner (or agent) has to achieve a specific goal by performing correct actions. If the goal is reached then he obtains reward. Given that there is no prior knowledge of which action is correct or wrong the agent needs to explore in order to reach the reward. It is possible for the agent to be rewarded not only when he reached the goal (future reward) but also by performing correct actions towards the goal (immediate reward).\n",
    "\n",
    "\n",
    "### Basic concepts and algorithms\n",
    "\n",
    "Reinforcement learning uses functions called value functions. Value functions are state-action pair functions that estimate how good a particular action will be in a given state, under a policy. Mathematically they are described by the notation $Q^{\\pi}(s,a)$.\n",
    "\n",
    "The value functions can be estimated by temporal difference (TD) learning methods. The TD methods calculate an estimation of the final reward at each state and the state-action value is updated for every step. They can be split up into two broad categories of methods, the on-policy methods and the off-policy methods. In the first category the agent learns the value of the policy that is used to make decisions, while in the second category the agent can learn different policies for behaviour and estimation. An example of an off-policy algorithm for TD learning is the Q-learning algorithm while an example of an on-policy algorithm for TD learning is the SARSA algorithm. The procedural forms of these two algorithms are described below:\n",
    "\n",
    "<img src=\"./algoQ.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"./algoSARSA.png\" width=\"500\"/>\n",
    "\n",
    "where $\\eta$ is the learning rate, $\\gamma$ is the discount factor and $max_a$ is the maximum reward that is attainable in the state following the current one.\n",
    "\n",
    "\n",
    "### The $\\epsilon$-greedy policy\n",
    "\n",
    "SARSA, being an on-policy algorithm does not explore new actions but always chooses the action with the highest expected reward. A way to include exploration is to choose the action with the highest expected reward at first, and then have a probability of following the same action or another, random one, the rest of the time. This approach is achieved with the $\\epsilon$-greedy policy where a greedy action is selected with probability $1-\\epsilon$, where $0 \\leq \\epsilon \\leq 1$, and a random action is selected the rest of the times.\n",
    "\n",
    "\n",
    "### Eligibility trace and the SARSA($\\lambda$) algorithm \n",
    "\n",
    "When TD learning algorithms like SARSA observes a high or low reward, the only state-action pair which has its $Q(s,a)$ value updated is the state-action pair $(s,a)$ which leads to the reward. The next time the algorithm reaches state $s$, the state-action pair which leads to state $s$ will have its Q-value updated. The idea now is to update all Q-values of the trajectory once the reward is received. In order for this to be achieved an eligibility trace $\\epsilon(s,a)$ needs to be maintained for each state-action pair. This trace indicates how much the current reward will influence $Q(s,a)$. The eligibility trace can be used in combination with the SARSA algorithm. The procedural form of the new algorithm named SARSA($\\lambda$) is described below:\n",
    "\n",
    "<img src=\"./algoSARSAl.png\" width=\"500\"/>\n",
    "\n",
    "where $\\eta$ is the learning rate, $\\gamma$ is the discount factor and $\\lambda$ is an indicator for the influence of the current reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 8\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "<img src=\"./robot.png\" width=\"500\"/>\n",
    "\n",
    "A robot moves in a square room with four possible actions, North, South, East and West. It has to learn a ”homing” task, i.e. to return to a particular location, where for instance it can charge its battery (reward location). There are no explicit landmarks, but to simplify the task we assume that the robot has been familiarised with the environment and therefore it has some internal representation of its position own position in the space, but no explicit memory of the reward location. Your task is to write a program where the above-mentioned goal-oriented behaviour (homing) can be learned by using a reinforcement learning algorithm, in the following way:\n",
    "\n",
    "1. The robot is placed at a random location (segment) of the room.\n",
    "2. It explores the space/learns the goal oriented behaviour by using the SARSA algorithm with Q-values.\n",
    "3. Reward is given when the robot reaches the segment where the charger is located.\n",
    "3. Trial ends when reward is reached (or a predefined number of steps is exceeded) and procedure starts from 1 until a predefined number of trials is reached.\n",
    "\n",
    "At the end, you should plot the number of steps it took the robot to reach its target vs the trial number (learning curve). Successful learning means that the required number of steps is reduced as the trial number increases. We will call this procedure one run of the algorithm.\n",
    "\n",
    "After developing the main routine (and confirming that your robot learns the desirable behaviour), use your code to study the properties of your model:\n",
    "\n",
    "1. Implement an eligibility trace. Plot the difference in performance.\n",
    "2. Find the optimal learning rate, discount factor and epsilon for the $\\epsilon$-Greedy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script that returns the learnign curve for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def homing_nn(n_trials,n_steps,learning_rate,eps,gamma):\n",
    "    ## Definition of the environment\n",
    "    N = 7                               #height of the gridworld ---> number of rows\n",
    "    M = 10                              #length of the gridworld ---> number of columns\n",
    "    N_states = N * M                    #total number of states\n",
    "    states_matrix = np.eye(N_states)\n",
    "    N_actions = 4                                           #number of possible actions in each state: 1->N 2->E 3->S 4->W\n",
    "    action_row_change = np.array([-1,0,+1,0])               #number of cell shifted in vertical as a function of the action\n",
    "    action_col_change = np.array([0,+1,0,-1])               #number of cell shifted in horizontal as a function of the action\n",
    "    End = np.array([1, 1])                                  #terminal state--->reward\n",
    "    s_end = np.ravel_multi_index(End,dims=(N,M),order='F')  #terminal state. Conversion in single index\n",
    "\n",
    "    ## Rewards\n",
    "    R = 10                              #only when the robot reaches the charger, sited in End state\n",
    "\n",
    "    ## Variables\n",
    "    weights = np.random.rand(N_actions,N_states)\n",
    "    learning_curve = np.zeros((1,n_trials))\n",
    "\n",
    "    ## SARSA\n",
    "\n",
    "    # Start trials\n",
    "    for trial in range(n_trials):\n",
    "\n",
    "        # Initialization\n",
    "        Start = np.array([np.random.randint(N),np.random.randint(M)])   #random start\n",
    "        s_start = np.ravel_multi_index(Start,dims=(N,M),order='F')      #conversion in single index\n",
    "        state = Start                                                   #set current state\n",
    "        s_index = s_start                                               #conversion in single index\n",
    "        step = 0\n",
    "\n",
    "        # Start steps\n",
    "        while s_index != s_end and step <= n_steps:\n",
    "\n",
    "            step += 1\n",
    "            learning_curve[0,trial] = step\n",
    "\n",
    "            input_vector = states_matrix[:,s_index].reshape(N_states,1)         #convert the state into an input vector\n",
    "\n",
    "            #compute Qvalues. Qvalue=logsig(weights*input). Qvalue is 2x1, one value for each output neuron\n",
    "            Q = 1 / ( 1 + np.exp( - weights.dot(input_vector)))    #Qvalue is 2x1 implementation of logsig\n",
    "\n",
    "            #eps-greedy policy implementation\n",
    "            greedy = (np.random.rand() > eps)               #1--->greedy action 0--->non-greedy action\n",
    "            if greedy:\n",
    "                action = np.argmax(Q)                           #pick best action\n",
    "            else:\n",
    "                action = np.random.randint(N_actions)           #pick random action\n",
    "\n",
    "\n",
    "            state_new = np.array([0,0])\n",
    "            #move into a new state\n",
    "            state_new[0] = state[0] + action_row_change[action]\n",
    "            state_new[1] = state[1] + action_col_change[action]\n",
    "\n",
    "            #put the robot back in grid if it goes out. Consider also the option to give a negative reward\n",
    "            if state_new[0] < 0:\n",
    "                state_new[0] = 0\n",
    "            if state_new[0] >= N:\n",
    "                state_new[0] = N-1\n",
    "            if state_new[1] < 0:\n",
    "                state_new[1] = 0\n",
    "            if state_new[1] >= M:\n",
    "                state_new[1] = M-1\n",
    "\n",
    "            s_index_new = np.ravel_multi_index(state_new,dims=(N,M),order='F')  #conversion in a single index\n",
    "\n",
    "            ## TODO update Qvalues. Only if is not the first step\n",
    "\n",
    "            #store variables for sarsa computation in the next step\n",
    "            output = np.zeros((N_actions,1))\n",
    "            output[action] = 1\n",
    "\n",
    "            #update variables\n",
    "            input_old = input_vector\n",
    "            output_old = output\n",
    "            Q_old = Q[action]\n",
    "            r_old = 0\n",
    "\n",
    "            state[0] = state_new[0]\n",
    "            state[1] = state_new[1]\n",
    "            s_index = s_index_new\n",
    "\n",
    "            ## TODO: check if state is terminal and update the weights consequently\n",
    "            if s_index == s_end:\n",
    "                pass\n",
    "\n",
    "\n",
    "    return learning_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function homing_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51., 51., 11., 51., 51., 51.,  2., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 21., 51., 51., 51., 39., 51., 51., 13., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 19., 51.,\n",
       "        51., 51., 44., 51., 51., 51., 51., 51.,  7., 51., 51., 51., 51.,\n",
       "        15., 51., 51.,  1., 51., 51., 51., 51., 51., 51., 51., 51., 18.,\n",
       "        26., 51., 26., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 11., 51.,  3., 51., 51., 51., 51.,  8., 49.,\n",
       "        51., 51., 51., 51., 51., 51.,  1., 51., 51.,  4., 51.,  4., 51.,\n",
       "        51.,  3., 17.,  6., 51., 51., 51., 51., 51.,  3., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 25., 51., 51., 48., 51., 51., 20., 51.,\n",
       "        51.,  4., 51., 51., 51.,  2., 10., 51.,  4., 51., 51., 41., 13.,\n",
       "        51., 51., 51.,  3., 51., 51., 29., 51., 51.,  1., 51., 51., 51.,\n",
       "        51., 51., 51.,  2., 51., 28., 30., 51., 51., 51., 51.,  3., 27.,\n",
       "        25., 51., 51., 51., 51., 51.,  2., 20., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 37., 51., 51., 51., 51.,  1., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51.,  1., 17., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51.,  2., 51., 51., 51., 20., 51., 51.,\n",
       "        51., 51.,  2., 51., 51., 51., 51.,  4., 51., 51., 51., 51., 51.,\n",
       "        51., 51.,  3., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 13.,  4., 51., 51., 51., 51., 51., 36., 51., 51.,  1., 51.,\n",
       "        51., 51., 51.,  3., 51., 51., 51., 51., 51., 51., 51., 51., 34.,\n",
       "        51., 51.,  1., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,  3.,  0., 51.,\n",
       "        42., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,  3., 51.,\n",
       "        51., 51., 51., 51., 51.,  2.,  1., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51.,  6., 51., 51., 51., 51., 51., 51.,  2.,\n",
       "        51., 51., 51., 20., 51., 51., 51., 51., 51., 12., 51., 51., 38.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,  5., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 13., 51., 51., 51., 51., 51., 51.,\n",
       "        51.,  0., 51., 51., 51., 51., 51., 51.,  3., 13., 51., 27., 51.,\n",
       "        51., 20., 51.,  0., 51.,  3., 51., 51., 51., 40., 51., 51., 51.,\n",
       "        44., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 22., 51., 51., 35., 51., 51., 51., 51., 51., 51., 51.,  1.,\n",
       "        49., 51., 51.,  1., 51., 34., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 13., 51., 51., 51.,\n",
       "         2.,  3., 51., 51., 51.,  1.,  1., 51., 51., 51., 51., 51., 51.,\n",
       "        51.,  3., 51., 47., 51., 51.,  1., 51., 51., 51., 51., 51., 51.,\n",
       "        29., 51., 51., 15., 10., 51., 48.,  4.,  4., 51., 51.,  7., 51.,\n",
       "        51., 13., 51., 51., 51., 51., 51., 12.,  1., 51., 51., 26., 51.,\n",
       "        51., 16., 26., 46., 51.,  2., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        41., 51., 51., 51., 51., 51.,  2., 51., 51., 51., 51., 51., 51.,\n",
       "         4., 51., 51., 51.,  0., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 13., 51.,  2., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51.,  9., 51., 51., 51.,\n",
       "        51.,  8.,  4., 51., 51., 51., 51., 51., 51.,  2., 51., 51., 51.,\n",
       "         1., 43., 51., 51., 10., 51., 27.,  3., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 36., 51., 51., 51., 51.,  2., 51., 14.,\n",
       "        51., 51., 51., 51., 51.,  1., 51., 51., 51., 51.,  2., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51.,  1.,  5., 51.,  0., 51., 51., 51.,\n",
       "         2.,  1., 51., 51.,  0., 13., 51., 51., 51.,  1., 51.,  3., 51.,\n",
       "        51., 51., 51.,  7., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "         1., 51., 51., 22., 19., 51., 45., 51., 51., 51., 51.,  2., 51.,\n",
       "        51., 25., 51., 51., 22., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 28.,  0., 51., 29., 51.,\n",
       "        51., 51.,  1., 51., 51., 51., 51., 51., 39.,  3., 42., 51., 51.,\n",
       "         2., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        22., 51.,  6., 19., 51., 51., 24., 11., 11., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51.,  1.,  3., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,  5.,\n",
       "        51., 51.,  1., 51., 51., 51., 51., 51., 51.,  1., 51.,  1., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 28., 51., 33., 51.,\n",
       "        51., 51.,  2., 51., 51., 51., 51., 51., 51., 24.,  0., 51., 21.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,  1., 51.,\n",
       "        51., 51., 51., 51., 43.,  4., 51., 51., 51., 51., 51.,  1., 15.,\n",
       "        51.,  2., 51., 51., 51.,  2., 51., 51., 51., 51., 51.,  3., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51.,  5., 51., 51., 51.,\n",
       "        51., 51.,  6., 51.,  5.,  3., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        13., 51., 51., 51., 51., 40., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51.,  4., 39., 51., 51., 51.,  1., 51., 51., 51., 51., 51.,\n",
       "        27., 51., 21., 51.,  0., 51., 51., 51., 51., 51.,  2., 51., 51.,\n",
       "        51., 51., 51., 46., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51., 51.,\n",
       "        51., 51., 51.,  7., 51., 51., 51.,  2., 51., 51.,  3., 51.,  1.,\n",
       "        17.,  0., 51., 51.,  8.,  2., 51., 51., 51.,  2., 51., 51.,  1.,\n",
       "        51., 51., 51., 51., 51., 51., 51., 51., 40., 51., 11., 51., 51.,\n",
       "        51., 51.,  1., 51., 51., 51., 13., 32.,  2.,  0., 19., 51.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NBVAL_SKIP\n",
    "\n",
    "# Parameter setup\n",
    "nrepetitions = 15;  # number of runs for the algorithm\n",
    "nTrials = 1000;     # should be integer >0\n",
    "nSteps = 50;        # maximum number of allowed steps\n",
    "learningRate = 0.3; # should be real, Greater than 0\n",
    "epsilon = 0.25;     # should be real, Greater or Equal to 0; epsion=0 Greedy, otherwise epsilon-Greedy\n",
    "gamma = 0.9;        # should be real, positive, smaller than 1\n",
    "\n",
    "\n",
    "#TODO: average number of steps to finish the task per trial for nrepetitions\n",
    "#for i = 1:nrepetitions....\n",
    "homing_nn(nTrials,nSteps,learningRate,epsilon,gamma)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
